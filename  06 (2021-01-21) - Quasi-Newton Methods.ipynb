{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Modifications to Newton's Method\n",
    "\n",
    " We will talk about:\n",
    "* Making Newton's method globally convergent\n",
    "* Reducing the computational complexity of Newton's Method\n",
    "* Quasi-Newton methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Newton's Method\n",
    "\n",
    "Not satisfied with the linear convergence of steepest descent methods, last time we introduced **Newton's method**, defined by the iteration\n",
    "\n",
    "$$ \\mathbf{x}_{k+1} = \\mathbf{x}_k - \\left[\\nabla^2 f_k\\right]^{-1}\\nabla f_k $$\n",
    "\n",
    "which is a line search algorithm with $\\mathbf{p}_k = -\\left[\\nabla^2 f_k\\right]^{-1}\\nabla f_k$, called the **Newton direction**, and $\\alpha_k=1$. We showed several nice results, including that the method converges in *a single step* for quadratic functions and that the method converges quadratically for most others, provided our initial guess is \"close enough\" to the minimizer. There are, however, several drawbacks to Newton's method:\n",
    "1. First and foremost, it requires calculating not just the gradient but also the Hessian (at least ostensibly) by hand.\n",
    "2. Even if we can find a way to numerically calculate the Hessian, Newton's method also requires inverting the Hessian (or, more efficiently, solving a linear system) during each iteration, which can be costly, especially for high-dimensional functions.\n",
    "3. The Hessian may even be singular far away from the minimizer (though it can be shown the Hessian is nonsingular at least in a neighborhood of $\\mathbf{x}^*$), so Newton's method cannot even be defined globally; it is only **locally** convergent.\n",
    "4. Even if the Hessian is nonsingular at each point on the iteration, the Newton direction may not be a descent direction in general.\n",
    "\n",
    "Today we will discuss several modifications to Newton's method that attempt to remedy some or all of these problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Newton + Line Search\n",
    "\n",
    "The first problem we will seek to remedy is global convergence. If the Hessian $\\nabla^2f$ is not positive definite, the Newton direction may not be a descent direction. This is true since\n",
    "$$ \\mathbf{p}_k^T\\nabla f_k =\\left(-\\left[\\nabla^2 f_k\\right]^{-1}\\nabla f_k\\right)^T\\nabla f_k = -\\nabla f_k^T\\left[\\nabla^2 f_k\\right]^{-T}\\nabla f_k $$\n",
    "\n",
    "is not necessarily negative if $\\nabla^2 f_k$ is not positive definite. (Recall that the eigenvalues of the inverse of a matrix are the reciprocals of the eigenvalues of the original matrix, so $A^{-1}$ is SPD if and only if $A$ is SPD.) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will discuss non positive definite Hessians momentarily, but even if $\\nabla^2f$ is positive definite at a given point, it is possible that the \"natural\" step size $\\alpha_k=1$ definited by Newton's method is *too large*, and thus the iteration will \"jump out\" of a local valley and possibly increase the value of the function. One simple modification to Newton's method that guarantees descent (and thus global convergence) is to simply restore the variability of step size, i.e. define the iteration\n",
    "$$ \\mathbf{x}_{k+1} = \\mathbf{x}_k - \\color{red}{\\alpha_k}\\left[\\nabla^2 f_k\\right]^{-1}\\nabla f_k $$\n",
    "\n",
    "then we can apply any of the previously discussed algorithms for determining $\\alpha_k$ (exact line search, backtracking, small fixed step, etc.) to ensure convergence.\n",
    "\n",
    "In practice, if the Newton direction is not a descent direction for the first few iterations (i.e. we are \"too far away\" from a minimizer), we set $\\alpha_k$ to some small value (perhaps using backtracking) to ensure descent. As we continue the iteration, we will eventually be in a neighborhood of a local minimizer (\"close enough\"), so the Hessian is guaranteed to be positive definite, and we simply set $\\alpha_k=1$ to take advantage of the quadratic convergence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Levenberg-Marquardt Algorithm\n",
    "\n",
    "If $\\nabla^2 f$ is positive definite everywhere along the sequence, we can guarantee both global convergence (by e.g. backtracking $\\alpha_k$ from an initial value of 1 if necessary) and local *quadratic* convergence with Newton's method, so it makes sense to use that if possible. If $\\nabla^2 f$ is not positive definite, Newton's method is not guaranteed to converge, but we do not have to give up and fall back on the only linearly convergent steepest descent algorithm. Instead, the **Levenberg-Marquardt (LM) algorithm** defines a slight modification to Newton's method when $\\nabla^2f$ is not positive definite, then recovers Newton's method exactly when we are close enough to a minimizer to achieve positive definiteness. The algorithm is defined as follows:\n",
    "\n",
    "1. If $\\nabla^2 f_k$ is not positive definite, define $B_k=\\nabla^2 f_k + \\mu_k I$, where $I$ is the identity matrix and $\\mu_k$ is large enough to ensure $B_k$ is positive definite. If $\\nabla^2f_k$ *is* positive definite, define $B_k=\\nabla^2 f_k$, i.e. set $\\mu_k=0$.\n",
    "2. Define the iteration $\\mathbf{x}_{k+1} = \\mathbf{x}_k - \\alpha_kB_k^{-1}\\nabla f_k$.\n",
    "3. Repeat 1-2 until convergence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In essence, the LM algorithm replaces $\\nabla^2f_k$ with an *approximation* $B_k$ which is guaranteed to be positive definite, providing descent and thus convergence. Once the iteration achives positive definiteness of the Hessian, the approximation is no longer necessary, so the original Hessian is used. This algorithm works because of the fact that the eigenvalues of $A+\\mu I$ are simply the eigenvalues of $A$, shifted by $\\mu$. Thus if $\\nabla^2f_k$ has negative eigenvalues, we can simply set $\\mu_k=|\\lambda_{min}(\\nabla^2f_k)|+\\delta$ for some small $\\delta>0$.\n",
    "\n",
    "It should be clear that for $\\mu_k$ small enough, the approximation $B_k$ is close to $\\nabla^2f_k$, but what about when $\\mu_k$ is large? If $\\mu_k\\to\\infty$, the eigenvalues of $B_k$ are all huge, and thus the eigenvalues of $B_k^{-1}$ are all very small (the eigenvalues of the inverse of a matrix are the reciprocals of the eigenvalues of the original matrix). Thus in this limit, we essentially recover the steepest descent direction $\\mathbf{p}_k\\approx -\\nabla f_k$ with a small step size, guaranteeing descent, so there is no problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In practice, determining the eigenvalues of $\\nabla^2f_k$ is itself a computationally expensive task, but it is easy to check if the Newton direction is not a descent direction (implying the Hessian was not positive definite), so we can simply do this check each iteration (or for only the first few), and if it's not, set $\\mu_k$ to a large value to ensure descent. Then as more iterations are performed, decrease the value of $\\mu_k$ until it is no longer necessary. Indeed, we could even perform a kind of backtracking algorithm on $\\mu_k$, slowly decreasing it from an initial large value during a *single* iteration, stopping and undoing the last step when positive definiteness is no longer guaranteed. At a certain point, though, the eigenvalue decomposition becomes less expensive than multiple checks for descent, so we may be better off just biting the bullet and finding the decomposition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quasi-Newton Methods\n",
    "\n",
    "Both of the aforementioned modifications to Newton's method attempt to solve the global convergence issue, first by ensuring that the Hessian is positive definite (or replacing it with a positive definite approximation) and then by ensuring that the Newton (or approximate Newton) direction are legitimate descent directions. Neither of these modifications, however, attempt to solve the complexity issue. They both still require computation of the Hessian, and they both still require inverting the Hessian. The next group of iterative methods we will discuss, called **quasi-Newton methods**, go farther. The basic idea of quasi-Newton methods is to approximate the Hessian with some matrix positive definite matrix $B_k$ during each iteration, or even approximate the inverse of the Hessian directly. A quasi-Newton method takes the following form:\n",
    "\n",
    "$$\\mathbf{x}_{k+1} = \\mathbf{x}_k - \\alpha_kB_k^{-1}\\nabla f_k \\equiv \\mathbf{x}_k - \\alpha_kH_k\\nabla f_k$$\n",
    "\n",
    "where $B_k$ is a positive definite approximation of the Hessian $\\nabla^2 f_k$ and/or $H_k$ is a positive definite approximation of the inverse Hessian $[\\nabla^2f_k]^{-1}$. This form should look very familiar, as setting $B_k=\\nabla^2f_k+\\mu_kI$ is just the LM algorithm discussed above. Indeed, if $B_k=H_k=I$, even steepest descent can be considered a quasi-Newton method, just with a very, very crude \"approximation\" (if it can even be considered that) of the Hessian being just the identity matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the actual Hessian satisfies\n",
    "$$ \\nabla^2f_k(\\mathbf{x}_{k+1}-\\mathbf{x}_k)\\approx \\nabla f_{k+1}-\\nabla f_k $$\n",
    "\n",
    "we choose $B_k$ such that it has the same property, that is determine a solution to\n",
    "\n",
    "$$ B_{k+1}(\\mathbf{x}_{k+1}-\\mathbf{x}_k)= \\nabla f_{k+1}-\\nabla f_k,\\qquad\\text{or}\\qquad B_{k+1}\\mathbf{s}_k = \\mathbf{y}_k $$\n",
    "\n",
    "with $\\mathbf{s}_k\\equiv \\mathbf{x}_{k+1}-\\mathbf{x}_k$ and $\\mathbf{y}_k\\equiv\\nabla f_{k+1}-\\nabla f_k$. The above, called the **secant equation**, is equivalent to the following requirements for the inverse approximation, $H_{k+1}$, \n",
    "\n",
    "$$ H_{k+1}(\\nabla f_{k+1}-\\nabla f_k)=\\mathbf{x}_{k+1}-\\mathbf{x}_k,\\qquad\\text{or}\\qquad H_{k+1}\\mathbf{y}_k =\\mathbf{s}_k $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, solving either of these equations exactly is just as expensive as inverting the Hessian in the unmodified Newton's method, so it doesn't appear that we've really helped ourselves. The key feature of all quasi-Newton methods, though, is that they do not recalculate $H_{k+1}$ during each iteration; instead they rely on some recursive definition, determining $H_{k+1}$ by using some low-rank update to the old approimation $H_k$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Symmetric Rank-1 (SR1) Method\n",
    "\n",
    "The simplest quasi-Newton method is the **symmetric rank-1 (SR1) method** which requires updates to the inverse Hessian to be of the form\n",
    "\n",
    "$$ H_{k+1} = H_k + \\mathbf{z}_k\\mathbf{z}_k^T $$\n",
    "\n",
    "where $\\mathbf{z}_k$ is a vector chosen to satisfy the secant equation. Thus, at each time step, all that is required is a single matrix multiplication rather than inverting a matrix. Plugging this assumption into the secant equation and churning through the algebra leads to the requirement\n",
    "\n",
    "$$ \\mathbf{z}_k\\mathbf{z}_k^T = \\frac{(\\mathbf{s}_k - H_k \\mathbf{y}_k) (\\mathbf{s}_k - H_k \\mathbf{y}_k)^T}{(\\mathbf{s}_k - H_k \\mathbf{y}_k)^T \\mathbf{y}_k} $$\n",
    "\n",
    "Then, assuming we define a sufficient initial approximation $H_0$ to the inverse Hessian (e.g. running \"pure\" Newton once, or often simply setting $H_0=I$ to perform steepest descent), no systems need be solved, and we can perform quasi-Newton updates with roughly the same amount of calculation as a steepest descent calculation!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All together, then, the SR1 method can be described as follows:\n",
    "\n",
    "1. Set $H_0=I$, the identity matrix.\n",
    "2. Update $\\mathbf{x}_{k+1} = \\mathbf{x}_k - \\alpha_k H_k\\nabla f_k$.\n",
    "3. Define $\\mathbf{s}_k \\equiv \\mathbf{x}_{k+1}-\\mathbf{x}_k$, $\\mathbf{y}_k\\equiv\\nabla f_{k+1}-\\nabla f_k$.\n",
    "4. Set $H_{k+1} = H_k + \\dfrac{(\\mathbf{s}_k - H_k \\mathbf{y}_k) (\\mathbf{s}_k - H_k \\mathbf{y}_k)^T}{(\\mathbf{s}_k - H_k \\mathbf{y}_k)^T \\mathbf{y}_k}$.\n",
    "5. Repeat 2-4 until convergence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will discuss convergence properties in more detail later, but it can be shown that under certain conditions, SR1 converges not just superlinearly but *in a finite number of steps* ($n$ if $\\mathbf{x}\\in\\mathbb{R}^n$) if $f$ is a quadratic function. Though this is not as good as the \"pure\" Newton's method (which converges in a single step for quadratic funcitons), the result is indicative of the improvement SR1 is over steepest descent. In general quasi-Newton methods will perform better than steepest descent but not as well as pure Newton. That is, they will typically have super-linear but not quite quadratic convergence rates at worst, $1<p<2$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Drawbacks of SR1\n",
    "\n",
    "SR1 can encounter a rather fatal issue, specifically with the denominator of $\\mathbf{z}_k\\mathbf{z}_k^T$, where $(\\mathbf{s}_k-H_k\\mathbf{y}_k)^T\\mathbf{y}_k$ may vanish or be very small if the two vectors happen to be orthogonal or very close to it. This can be interpreted as an artifact of requiring a rank-1 solution to the secant equation, which may not exist. In practice, this happens rarely, but in case it does, the typical workaround is to simply skip updating that iteration, i.e. set $H_{k+1}=H_k$, if the magnitude of the denominator is smaller than some arbitrarily chosen constant, e.g. don't udpate if $|(\\mathbf{s}_k-H_k\\mathbf{y}_k)^T\\mathbf{y}_k|<10^{-8}\\|\\mathbf{s}_k-H_k\\mathbf{y}_k\\|\\|\\mathbf{y}_k\\|$. The hope is that in future iterations $\\mathbf{s}_k$ and $\\mathbf{y}_k$ are modified such that the problem is avoided. Empirically this does not appear to have very much effect on the convergence of the iteration.\n",
    "\n",
    "Another difficulty of SR1 is that the update to the inverse Hessian is not guaranteed to preserve positive definiteness. Because of this, we may not be guaranteed descent in general. We will discuss another quasi-Newton method next time that attempts to remedy both of these issues."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
