{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Convergence of Heavyball, Comparisons\n",
    "\n",
    "In this discussion, we will explore\n",
    "* Convergence rate of heavyball\n",
    "* Comparisons with other methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last time we introduced **heavyball method**, also referred to as **steepest descent with momentum**, defined by the following:\n",
    "\n",
    "$$ \\begin{align*}\n",
    "    \\mathbf{p}_k &= -\\alpha_k\\nabla f_k + \\beta \\mathbf{p}_{k-1} \\\\\n",
    "    \\mathbf{x}_{k+1} &= \\mathbf{x}_k + \\mathbf{p}_k\n",
    "\\end{align*} $$\n",
    "\n",
    "where $\\beta\\in(0,1)$ is called the *momentum parameter*. We showed an example of this method improving upon the convergence of steepest descent, but we didn't really give a mathematical justification for this. That is our goal today, but first let us recall a few facts about steepest descent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convergence rate of steepest descent\n",
    "\n",
    "We showed previously that, for quadratic functions, the fixed step steepest descent algorithm admits the inequality\n",
    "\n",
    "$$ \\frac{\\|\\mathbf{x}_{k+1}-\\mathbf{x}^*\\|}{\\|\\mathbf{x}_{k}-\\mathbf{x}^*\\|} \\le \\frac{\\lambda_{max}(Q)-\\lambda_{min}(Q)}{\\lambda_{max}(Q) + \\lambda_{min}(Q)} \\equiv \\frac{\\kappa-1}{\\kappa+1} $$\n",
    "\n",
    "provided $\\alpha$ is chosen optimally, where $\\kappa\\equiv \\frac{\\lambda_{max}(Q)}{\\lambda_{min}(Q)}\\ge 1$ is the condition number of $Q$. This inequality demonstrates the linear convergence of steepest descent for quadratic functions since if $\\kappa>1$, $\\frac{\\kappa-1}{\\kappa+1} = 1 - \\frac{2}{\\kappa+1}<1$. Recall also that this bound is  achieved by choosing\n",
    "\n",
    "$$\\alpha = \\frac{2}{\\lambda_{max}(Q) + \\lambda_{min}(Q)} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We remarked that this result could be extended to certain non-quadratic functions, provided they are \"nice\" enough. In fact, the strict requirement is that $f$ be a $C^2$ function that is **strongly convex** and **strongly smooth**. A function is strongly convex if\n",
    "\n",
    "$$ f(\\mathbf{y}) \\ge f(\\mathbf{x}) +\\nabla f(\\mathbf{x})^T(\\mathbf{y}-\\mathbf{x}) + \\frac{\\mu}{2}\\|\\mathbf{y}-\\mathbf{x}\\|^2 $$\n",
    "\n",
    "holds for all $\\mathbf{x},\\mathbf{y}\\in\\mathbb{R}^n$ and some constant $\\mu > 0$ (strictly!). A similar expression exists for strongly smooth, i.e. \n",
    "\n",
    "$$ f(\\mathbf{y}) \\le f(\\mathbf{x}) +\\nabla f(\\mathbf{x})^T(\\mathbf{y}-\\mathbf{x}) + \\frac{L}{2}\\|\\mathbf{y}-\\mathbf{x}\\|^2 $$\n",
    "\n",
    "for some $L\\ge\\mu$. The combination of these two inequalities are essentially putting strict upper ($L$) and lower ($\\mu$) bounds on the eigenvalues of the Hessian, rather than allowing arbitrarily small or large values (thus the \"strong\" qualifier). If $f$ is quadratic, $L=\\lambda_{max}(Q)$ and $\\mu=\\lambda_{min}(Q)$, i.e. all quadratic functions are strongly convex/smooth. Provided the bounds of $L$ and $\\mu$ are able to be determined for a non-quadratic function (which may be difficult in practice), the result above generalizes nicely: choosing\n",
    "\n",
    "$$ \\alpha = \\frac{2}{L+\\mu} \\implies \\frac{\\|\\mathbf{x}_{k+1}-\\mathbf{x}^*\\|}{\\|\\mathbf{x}_{k}-\\mathbf{x}^*\\|} \\le \\frac{L-\\mu}{L + \\mu} $$\n",
    "\n",
    "and so we still get linear convergence since $L\\ge\\mu$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convergence rate of Heavyball\n",
    "\n",
    "If $L\\gg\\mu$ above, the constant $r\\equiv \\frac{L-\\mu}{L+\\mu}$ is very close to 1, which can be interpreted as the error not improving very much each iteration, resulting in slow convergence. It is shown in lecture by an argument based on treating the iteration update as a [fixed point iteration](https://en.wikipedia.org/wiki/Fixed-point_iteration) that a very similar bound can be obtained for the heavyball update. If we choose\n",
    "\n",
    "$$ \\alpha = \\frac{4}{(L^{1/2} + \\mu^{1/2})^2},\\quad \\beta = \\frac{(L^{1/2} - \\mu^{1/2})^2}{(L^{1/2} + \\mu^{1/2})^2} \\qquad \\implies\\qquad  \\frac{\\|\\mathbf{x}_{k+1}-\\mathbf{x}^*\\|}{\\|\\mathbf{x}_{k}-\\mathbf{x}^*\\|} \\le \\frac{L^{1/2}-\\mu^{1/2}}{L^{1/2} + \\mu^{1/2}} $$\n",
    "\n",
    "which is the same as the above with the replacement $L\\to L^{1/2}$, $\\mu\\to\\mu^{1/2}$. For the same values of $L,\\mu$, the ratio $r$ for heavyball is usually smaller than the ratio $r$ for steepest descent, resulting in faster convergenge. Note that the convergence for heavyball is **still linear**, but since the ratio of consecutive errors is smaller, the iteration will converge more quickly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We demonstrate this difference below by comparing steepest descent to heavyball with these optimal fixed parameters on the (quadratic) function we looked at last time:\n",
    "\n",
    "$$ f(x,y) = x^2 + 10y^2 $$\n",
    "\n",
    "which has $L=20$, $\\mu=2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "f = lambda x,y: x**2 + 10*y**2\n",
    "df = lambda x,y: np.array([2*x, 20*y])\n",
    "Q = np.array([[2,0],[0,20]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "x = np.array([10,1])\n",
    "path_SD = [x]\n",
    "tol = 1e-8\n",
    "alpha = 2/(20+2)  # optimal step size\n",
    "max_steps = 1000\n",
    "dx = df(x[0],x[1])\n",
    "i=0\n",
    "while np.linalg.norm(dx)>tol and i<max_steps:\n",
    "    xnew = x - alpha*dx\n",
    "    path_SD.append(xnew)\n",
    "    x = xnew\n",
    "    i += 1\n",
    "    dx = df(x[0],x[1])\n",
    "    \n",
    "path_SD=np.array(path_SD)\n",
    "print(f'After {i} iterations, approximate minimum is {f(x[0],x[1])} at {x}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_path(path, func, title, window=[-10,10,-5,5], contours=50, skip=1):\n",
    "    '''Plots path defined in (N,2) array \"path\" on a contour plot of \"func\" in window \"window\"'''\n",
    "    plt.figure(figsize=(10,5),dpi=150)\n",
    "    X = np.linspace(window[0],window[1],300)\n",
    "    Y = np.linspace(window[2],window[3],300)\n",
    "    Xmesh, Ymesh = np.meshgrid(X,Y)\n",
    "    Z = func(Xmesh,Ymesh)\n",
    "    CS = plt.contour(Xmesh, Ymesh, Z, contours, cmap='jet')\n",
    "    plt.clabel(CS,inline_spacing=0,fmt='%d')\n",
    "    plt.axis(window)\n",
    "    plt.xlabel('x')\n",
    "    plt.ylabel('y')\n",
    "    plt.title(title)\n",
    "\n",
    "    for i in range(path.shape[0]-1): # iterate through steps\n",
    "        if i%skip==0:\n",
    "            # only plot arrows every \"skip\" iterations\n",
    "            plt.arrow(path[i,0],path[i,1],path[i+1,0]-path[i,0],path[i+1,1]-path[i,1],\n",
    "                      color='k',length_includes_head=True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_path(path_SD,f,'Steepest descent', contours=np.arange(1,20)**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see this takes quite a few iterations even for a quadratic function. If we investigate the ratio\n",
    "\n",
    "$$ r = \\frac{L-\\mu}{L+\\mu} = \\frac{20-2}{20+2} \\approx 0.81818 $$\n",
    "\n",
    "we see the error in each iteration is still more than $80\\%$ of the previous iteration, quite large. We write a function below to determine this value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def approx_error_ratio(path, minimizer, numToAvg=10, showPlot=False):\n",
    "    '''Given a path defined by an iteration and a known minimizer, approximates the limit of the ratio of errors'''\n",
    "    err = np.linalg.norm(path-np.array(minimizer),axis=1) # ||x_k-x*||=e_k\n",
    "    \n",
    "    # if converged in very few steps, return infinite order\n",
    "    if len(err)<=2:\n",
    "        return np.inf\n",
    "    \n",
    "    rr = np.zeros(len(err)-2)\n",
    "    for i in range(len(err)-2):\n",
    "        rr[i] = err[i+1]/err[i]\n",
    "    \n",
    "    if numToAvg>len(rr):\n",
    "        # if not enough iterations to average, just average all\n",
    "        r = np.mean(rr)\n",
    "    else:\n",
    "        # return mean of last few iterations\n",
    "        r = np.mean(rr[-numToAvg:])\n",
    "        \n",
    "    # plot\n",
    "    if showPlot:\n",
    "        plt.plot(rr)\n",
    "        plt.plot(rr*0+r)\n",
    "        plt.xlabel('k')\n",
    "        plt.ylabel('r')\n",
    "        plt.title(f'r={r}')\n",
    "        plt.show()\n",
    "        \n",
    "    return r, rr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_SD, rr_SD = approx_error_ratio(path_SD,[0,0],showPlot=True)\n",
    "print('r =',r_SD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now doing the same using heavyball, we should expect faster convergence, since\n",
    "\n",
    "$$ r = \\frac{L^{1/2}-\\mu^{1/2}}{L^{1/2}+\\mu^{1/2}} \\approx 0.51949 $$\n",
    "\n",
    "meaning each consecutive error is only about $52\\%$ of the previous error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "x = np.array([10,1])\n",
    "path_HB = [x]\n",
    "tol = 1e-8\n",
    "beta = (np.sqrt(20)-np.sqrt(2))**2/(np.sqrt(20)+np.sqrt(2))**2  # optimal momentum parameter\n",
    "alpha = 4/(np.sqrt(20)+np.sqrt(2))**2                           # optimal step size\n",
    "max_steps = 1000\n",
    "dx = df(x[0],x[1])\n",
    "pk = dx*0      # first update is just gradient descent\n",
    "i=0\n",
    "while np.linalg.norm(dx)>tol and i<max_steps:\n",
    "    pk = -alpha*dx + beta*pk\n",
    "    xnew = x + pk\n",
    "    path_HB.append(xnew)\n",
    "    x = xnew\n",
    "    i += 1\n",
    "    dx = df(x[0],x[1])\n",
    "\n",
    "path_HB = np.array(path_HB)\n",
    "print(f'After {i} iterations, approximate minimum is {f(x[0],x[1])} at {x}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_path(path_HB,f,'Heavyball', contours=np.arange(1,20)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_HB, rr_HB = approx_error_ratio(path_HB,[0,0],showPlot=True)\n",
    "print('r =',r_HB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(rr_HB[-5:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, the above is a very ideal example where the values of $L$ and $\\mu$ are easily calculable since the function is quadratic. If we apply a more sophisticated method such as Newton's, we expect convergence in one step. Even conjugate gradient, which is very similar to heavyball (indeed it can be described as heavyball with \"adaptive\" parameters $\\alpha$ and $\\beta$), is expected to converge in at most two steps since this is a function in $\\mathbb{R}^2$. We show this below, choosing the Fletcher-Reeves (FR) conjugate gradient step, recognizing that for quadratic functions, this just simplifies to the linear conjugate gradient method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FR_beta(Dfk,Dfk1):\n",
    "    return (Dfk1@Dfk1) / (Dfk@Dfk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "x = np.array([10,1])\n",
    "path_FR = [x]\n",
    "tol = 1e-8\n",
    "max_steps = 1000\n",
    "dx = df(x[0],x[1])\n",
    "pk = -dx    # first step is steepest descent\n",
    "i=0\n",
    "while np.linalg.norm(dx)>tol and i<max_steps:    \n",
    "    alpha = -(pk@dx)/(pk@(Q@pk)) # use optimal alpha\n",
    "    xnew = x + alpha*pk\n",
    "    dx1 = df(xnew[0],xnew[1])\n",
    "    bk = FR_beta(dx,dx1)\n",
    "    if i>0 and i%3 ==0:\n",
    "        bk = 0    # restarting\n",
    "    pk = -dx1 + bk*pk\n",
    "\n",
    "    path_FR.append(xnew)\n",
    "    x = xnew\n",
    "    dx = dx1\n",
    "    i += 1\n",
    "\n",
    "path_FR=np.array(path_FR)\n",
    "print(f'After {i} iterations, approximate minimum is {f(x[0],x[1])} at {x}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_path(path_FR,f,'Conjugate gradient', contours=np.arange(1,20)**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-quadratic example\n",
    "\n",
    "Since conjugate gradient and heavyball are so similar, an interesting question (which you will explore in homework) is which performs better on non-quadratic functions. Just to spice things up, let us choose a different non-quadratic function than we usually choose, the [Beale function](http://benchmarkfcns.xyz/benchmarkfcns/bealefcn.html), defined by\n",
    "\n",
    "$$ f(x,y) = (1.5-x+xy)^2 + (2.25-x+xy^2)^2 + (2.625-x+xy^3)^2 $$\n",
    "\n",
    "This function has a global minimum at $(3,0.5)$, but it has several non-ideal features like as saddle points ($\\nabla f=\\mathbf{0}$ but not a minimum), local suboptimal minima ($\\nabla f = \\mathbf{0}$ but better solutions exist), and a very flat plateau ($\\nabla f\\approx\\mathbf{0}$ so steps are small) near the global minimum. Let's make a contour plot of this function below to investigate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = lambda x, y: (1.5 - x + x*y)**2 + (2.25 - x + x*y**2)**2 + (2.625 - x + x*y**3)**2\n",
    "\n",
    "# numerical approximation of gradient using central difference; aint nobody got time for that\n",
    "h = 1e-6\n",
    "dg = lambda x, y: np.array([0.5*(g(x+h, y)-g(x-h, y))/h,\n",
    "                            0.5*(g(x, y+h)-g(x, y-h))/h])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.colors import LogNorm  # useful for visualizing this function\n",
    "\n",
    "def plot_path_Beale(path, func, title, window=[-4.5,4.5,-4.5,4.5], contours=np.logspace(-2,6,100), skip=1):\n",
    "    '''Plots path defined in (N,2) array \"path\" on a contour plot of \"func\" in window \"window\"'''\n",
    "    plt.figure(figsize=(5,5),dpi=150)\n",
    "    X = np.linspace(window[0],window[1],300)\n",
    "    Y = np.linspace(window[2],window[3],300)\n",
    "    Xmesh, Ymesh = np.meshgrid(X,Y)\n",
    "    Z = func(Xmesh,Ymesh)\n",
    "    CS = plt.contourf(Xmesh, Ymesh, Z, contours, cmap='jet', norm = LogNorm())\n",
    "    plt.axis(window)\n",
    "    plt.xlabel('x')\n",
    "    plt.ylabel('y')\n",
    "    plt.title(title)\n",
    "\n",
    "    if path is not None:\n",
    "        for i in range(path.shape[0]-1): # iterate through steps\n",
    "            if i%skip==0:\n",
    "                # only plot arrows every \"skip\" iterations\n",
    "                plt.arrow(path[i,0],path[i,1],path[i+1,0]-path[i,0],path[i+1,1]-path[i,1],\n",
    "                          color='k',length_includes_head=True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_path_Beale(None,g,'Beale function') # plot with no path to just show contour plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's explore how regular steepest descent performs on this function. One way we could attempt to determine an optimal step size for this function would be to determine its Hessian and see if we can bound the eigenvalues. Another (lazier but much faster) method might just be to simply evaluate (or even approximate) the Hessian only at the known minimizer $(3,0.5)$ and determine eigenvalues. Let's do that one, the easier one!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d2g = lambda x, y: np.array([[0.5*(dg(x+h, y)[0]-dg(x-h, y)[0])/h, 0.5*(dg(x, y+h)[0]-dg(x, y-h)[0])/h],\n",
    "                             [0.5*(dg(x+h, y)[1]-dg(x-h, y)[1])/h, 0.5*(dg(x, y+h)[1]-dg(x, y-h)[1])/h]])\n",
    "\n",
    "d2g(3,0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eigs, _ = np.linalg.eigh(d2g(3,0.5))\n",
    "print(eigs)\n",
    "\n",
    "print('alpha = ',4/(np.sqrt(eigs[0])+np.sqrt(eigs[1]))**2)\n",
    "print('beta = ',(np.sqrt(eigs[1])-np.sqrt(eigs[0]))**2/(np.sqrt(eigs[1])+np.sqrt(eigs[0]))**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try this step size below, first with steepest descent, then do the same thing using heavyball, both starting from the initial point $(-3,-1)$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "x = np.array([-3,-1])\n",
    "path_SD1_Beale = [x]\n",
    "tol = 1e-8\n",
    "alpha = 4/(np.sqrt(eigs[0])+np.sqrt(eigs[1]))**2   # optimal(?) step size\n",
    "max_steps = 20000\n",
    "dx = dg(x[0],x[1])\n",
    "i=0\n",
    "while np.linalg.norm(dx)>tol and i<max_steps:\n",
    "    xnew = x - alpha*dx\n",
    "    path_SD1_Beale.append(xnew)\n",
    "    x = xnew\n",
    "    i += 1\n",
    "    dx = dg(x[0],x[1])\n",
    "\n",
    "path_SD1_Beale = np.array(path_SD1_Beale)\n",
    "print(f'After {i} iterations, approximate minimum is {g(x[0],x[1])} at {x}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_path_Beale(path_SD1_Beale,g,'Steepest descent on Beale function, first attempt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well that didn't work. Does heavyball do any better?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "x = np.array([-3,-1])\n",
    "path_HB1_Beale = [x]\n",
    "tol = 1e-8\n",
    "beta = (np.sqrt(eigs[1])-np.sqrt(eigs[0]))**2/(np.sqrt(eigs[1])+np.sqrt(eigs[0]))**2  # optimal(?) momentum parameter\n",
    "alpha = 4/(np.sqrt(eigs[0])+np.sqrt(eigs[1]))**2                                      # optimal(?) step size\n",
    "max_steps = 1000\n",
    "dx = dg(x[0],x[1])\n",
    "pk = dx*0      # first update is just gradient descent\n",
    "i=0\n",
    "while np.linalg.norm(dx)>tol and i<max_steps:\n",
    "    pk = -alpha*dx + beta*pk\n",
    "    xnew = x + pk\n",
    "    path_HB1_Beale.append(xnew)\n",
    "    x = xnew\n",
    "    i += 1\n",
    "    dx = dg(x[0],x[1])\n",
    "\n",
    "path_HB1_Beale = np.array(path_HB1_Beale)\n",
    "print(f'After {i} iterations, approximate minimum is {g(x[0],x[1])} at {x}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_path_Beale(path_HB1_Beale,g,'Heavyball on Beale function, first attempt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[That's a negative, ghost rider; the pattern is full.](https://i.imgur.com/NlV7ISw.jpeg)\n",
    "\n",
    "Maybe a different value of $\\alpha$ and $\\beta$ would be better? Rather than just guessing (which tbh is the most commonly employed strategy), let's still try to stick to some semblance of rigor and do a [Monte Carlo approximation](https://en.wikipedia.org/wiki/Monte_Carlo_method) of the largest and smallest eigenvalues in our viewing window of $[-4.5,4.5]\\times[-4.5,4.5]$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxval = eigs[1]\n",
    "minval = eigs[0]\n",
    "for i in range(1000):\n",
    "    x = np.random.uniform(-4.5,4.5,2) # generate a random point\n",
    "    H = d2g(x[0],x[1])                # and evaluate the Hessian there\n",
    "    tmp_eigs, _ = np.linalg.eigh(H)\n",
    "    if tmp_eigs[1]>maxval:                # if eigenvalues larger or smaller, choose those\n",
    "        maxval = tmp_eigs[1]\n",
    "    if tmp_eigs[0]<minval:\n",
    "        minval = tmp_eigs[0]\n",
    "        \n",
    "print('Maximum eigenvalue :',maxval,'\\nMinimum eigenvalue :',minval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since there are some *concave* points in our domain, we have negative eigenvalues. Promptly ignoring that, let's simply use the maximum eigenvalue from above as part of our calculations, keeping the smaller eigenvalue from the minimizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "x = np.array([-3,-1])\n",
    "path_SD2_Beale = [x]\n",
    "tol = 1e-8\n",
    "alpha = 4/(np.sqrt(eigs[0])+np.sqrt(maxval))**2   # optimal(?) step size\n",
    "print('alpha = ',alpha)\n",
    "max_steps = 20000\n",
    "dx = dg(x[0],x[1])\n",
    "i=0\n",
    "while np.linalg.norm(dx)>tol and i<max_steps:\n",
    "    xnew = x - alpha*dx\n",
    "    path_SD2_Beale.append(xnew)\n",
    "    x = xnew\n",
    "    i += 1\n",
    "    dx = dg(x[0],x[1])\n",
    "\n",
    "path_SD2_Beale = np.array(path_SD2_Beale)\n",
    "print(f'After {i} iterations, approximate minimum is {g(x[0],x[1])} at {x}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_path_Beale(path_SD2_Beale,g,'Steepest descent on Beale function, second attempt', skip=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well at least we're moving now, but 1) This is taking way too long, and 2) We don't appear to be converging to the right place! How about heavyball?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "x = np.array([-3,-1])\n",
    "path_HB2_Beale = [x]\n",
    "tol = 1e-8\n",
    "beta = (np.sqrt(maxval)-np.sqrt(eigs[0]))**2/(np.sqrt(maxval)+np.sqrt(eigs[0]))**2  # optimal(?) momentum parameter\n",
    "print('beta = ',beta)\n",
    "alpha = 4/(np.sqrt(eigs[0])+np.sqrt(maxval))**2                                      # optimal(?) step size\n",
    "print('alpha = ',alpha)\n",
    "max_steps = 20000\n",
    "dx = dg(x[0],x[1])\n",
    "pk = dx*0      # first update is just gradient descent\n",
    "i=0\n",
    "while np.linalg.norm(dx)>tol and i<max_steps:\n",
    "    pk = -alpha*dx + beta*pk\n",
    "    xnew = x + pk\n",
    "    path_HB2_Beale.append(xnew)\n",
    "    x = xnew\n",
    "    i += 1\n",
    "    dx = dg(x[0],x[1])\n",
    "\n",
    "path_HB2_Beale = np.array(path_HB2_Beale)\n",
    "print(f'After {i} iterations, approximate minimum is {g(x[0],x[1])} at {x}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_path_Beale(path_HB2_Beale,g,'Heavyball on Beale function, second attempt', skip=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hey! We finally converged! This ad hoc method of choosing $\\alpha$ and $\\beta$ actually worked! Actually, if you investigate the path above, though, it appears we were already close to the minimum within only 1000 steps; the last 9000+ steps were all jostling around inside the valley. Perhaps we can modify the value of $\\alpha$ or $\\beta$ to get better convergence?\n",
    "\n",
    "**Exericise:** Try that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's look at the nonlinear conjugate method with an FR update. Although FR should use an adaptive $\\alpha$, let's try conjugate gradient with a fixed step size to see a somewhat fair comparison:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "x = np.array([-3,-1])\n",
    "path_FR1_Beale = [x]\n",
    "tol = 1e-8\n",
    "alpha = 4/(np.sqrt(eigs[0])+np.sqrt(maxval))**2\n",
    "max_steps = 20000\n",
    "dx = dg(x[0],x[1])\n",
    "pk = -dx    # first step is steepest descent\n",
    "i=0\n",
    "while np.linalg.norm(dx)>tol and i<max_steps:\n",
    "    xnew = x + alpha*pk\n",
    "    dx1 = dg(xnew[0],xnew[1])\n",
    "    bk = FR_beta(dx,dx1)    # still use adaptive beta\n",
    "    if i>0 and i%3 ==0:\n",
    "        bk = 0    # restarting\n",
    "    pk = -dx1 + bk*pk\n",
    "\n",
    "    path_FR1_Beale.append(xnew)\n",
    "    x = xnew\n",
    "    dx = dx1\n",
    "    i += 1\n",
    "\n",
    "path_FR1_Beale=np.array(path_FR1_Beale)\n",
    "print(f'After {i} iterations, approximate minimum is {g(x[0],x[1])} at {x}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_path_Beale(path_FR1_Beale,g,'FR on Beale function, fixed step size', skip=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that with the same fixed step size, even if $\\beta$ is calculated adaptively, conjugate gradient does not perform well, indeed about the same as steepest descent. However, below, we unleash the full power of conjugate gradient, allowing both $\\alpha$ and $\\beta$ to be calculated adaptively, here by using backtracking on alpha:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def WolfeI(alpha,f,x,dx,p,c1=0.1):\n",
    "    '''Return True/False if Wolfe condition I is satisfied for the given alpha'''\n",
    "    LHS = f(x[0]+alpha*p[0], x[1]+alpha*p[1])\n",
    "    RHS = f(x[0],x[1])+c1*alpha*np.dot(dx,p)\n",
    "    return LHS <= RHS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "x = np.array([-3,-1])\n",
    "path_FR2_Beale = [x]\n",
    "tol = 1e-8\n",
    "rho = 0.9  # rho for backtracking\n",
    "max_steps = 10000\n",
    "dx = dg(x[0],x[1])\n",
    "pk = -dx    # first step is steepest descent\n",
    "i=0\n",
    "while np.linalg.norm(dx)>tol and i<max_steps:\n",
    "    # backtracking\n",
    "    alpha = 1\n",
    "    while not WolfeI(alpha,g,x,dx,pk) and alpha>1e-5: # lower bound on alpha attempts to force Wolfe II\n",
    "        alpha *= rho\n",
    "        \n",
    "    xnew = x + alpha*pk\n",
    "    dx1 = dg(xnew[0],xnew[1])\n",
    "    bk = FR_beta(dx,dx1)\n",
    "    if i>0 and i%3 ==0:\n",
    "        bk = 0    # restarting\n",
    "    pk = -dx1 + bk*pk\n",
    "\n",
    "    path_FR2_Beale.append(xnew)\n",
    "    x = xnew\n",
    "    dx = dx1\n",
    "    i += 1\n",
    "\n",
    "path_FR2_Beale=np.array(path_FR2_Beale)\n",
    "print(f'After {i} iterations, approximate minimum is {g(x[0],x[1])} at {x}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_path_Beale(path_FR2_Beale,g,'FR on Beale function, adaptive step size')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I recommend playing around with the different parameters above to see how they affect convergence. What happens if we change $\\rho$ in backtracking? What about $c_1$ in the Wolfe I condition? Especially with such a non-convex function, this is an open question. There are no rules or expected results; simply exploration."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
