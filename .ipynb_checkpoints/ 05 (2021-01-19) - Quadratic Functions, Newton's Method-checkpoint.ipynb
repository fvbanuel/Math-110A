{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Quadratic Functions, Newton's Method\n",
    "\n",
    " We will talk about:\n",
    "* Rate of convergence for steepest descent on quadratic functions\n",
    "* Newton's method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rate of Convergence\n",
    "\n",
    "Last week, we defined the rate of convergence $p\\ge1$ to be the value such that\n",
    "\n",
    "$$ 0< \\lim_{k\\to\\infty} \\frac{\\|\\mathbf{x}_{k+1}-\\mathbf{x}^*\\|}{\\|\\mathbf{x}_k-\\mathbf{x}^*\\|^p} \\equiv L < \\infty $$\n",
    "\n",
    "where $\\mathbf{x}^*$ is the known minimizer, i.e. the terminal point satisfying $\\nabla f(\\mathbf{x}^*)=\\mathbf{0}$. If $p=1$ and $L=1$, we say the convergence is **sub-linear**. If $p=1$ and $L<1$, we say the convergence is **linear**. If $p>1$, we say the convergence is **super-linear**. If the limit is equal to 0 for all $p\\ge1$, we say the order of convergence is $\\infty$.\n",
    "\n",
    "We also showed that given the steps $\\{x_k\\}$ of an iteration, we can determine $p$ empirically by\n",
    "\n",
    "$$ p\\approx \\frac{\\ln\\left(\\frac{\\|\\mathbf{x}_{k+1}-\\mathbf{x}_k\\|}{\\|\\mathbf{x}_k-\\mathbf{x}_{k-1}\\|}\\right)}{\\ln\\left(\\frac{\\|\\mathbf{x}_{k}-\\mathbf{x}_{k-1}\\|}{\\|\\mathbf{x}_{k-1}-\\mathbf{x}_{k-2}\\|}\\right)} $$\n",
    "as $k\\to\\infty$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While this seems to be some abstract definition that can only be determined after the fact, we have shown some nice analytic results for a special class of functions called **quadratic functions**, i.e. those of the form\n",
    "\n",
    "$$ f(\\mathbf{x}) = \\frac{1}{2}\\mathbf{x}^TQ\\mathbf{x}-\\mathbf{b}^T\\mathbf{x} $$\n",
    "\n",
    "for $\\mathbf{b}\\in\\mathbb{R}^d$, $Q$ a symmetric positive definite $d\\times d$ matrix. This may seem like an obscure function, but most of the example functions we have used to this point have been functions of this type."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Quadratic function\n",
    "\n",
    "Note that if $\\mathbf{x}\\in\\mathbb{R}^2$, we have\n",
    "$$ \\begin{align*}\n",
    "    \\frac{1}{2}\\mathbf{x}^TQ\\mathbf{x}-\\mathbf{b}^T\\mathbf{x} &= \\frac{1}{2}\\begin{pmatrix} x_1 & x_2 \\end{pmatrix}\n",
    "    \\begin{pmatrix}\n",
    "        Q_{1,1} & Q_{1,2} \\\\\n",
    "        Q_{1,2} & Q_{2,2}\n",
    "    \\end{pmatrix}\\begin{pmatrix} x_1 \\\\ x_2\\end{pmatrix} - \\begin{pmatrix} b_1 & b_2\\end{pmatrix}\\begin{pmatrix} x_1 \\\\x_2 \\end{pmatrix} \\\\\n",
    "    & = \\frac{1}{2}Q_{1,1}x_1^2 + \\frac{1}{2}Q_{2,2}x_2^2 + Q_{1,2}x_1x_2 - b_1x_1 - b_2x_2\n",
    "\\end{align*}$$\n",
    "so, using our tried and true example function\n",
    "\n",
    "$$ g(x,y) = (x-1)^2+(2y-1)^2 = x^2+4y^2 - 2x - 4y + 2 $$\n",
    "\n",
    "we can identify\n",
    "$$ Q=\\begin{pmatrix} 2 & 0 \\\\ 0 & 8 \\end{pmatrix},\\quad \\mathbf{b}=\\begin{pmatrix} 2 \\\\ 4 \\end{pmatrix} $$\n",
    "\n",
    "with the minimization of the function $f(x,y)\\equiv g(x,y)-2$. Note that the subtraction of a constant will not affect the location of the minimizer, $\\mathbf{x}^*=(1,1/2)$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Steepest Descent on Quadratic Functions\n",
    "\n",
    "Thanks to the nice form of quadratic functions, there are some reasonably straightforward theoretical convergence results that can be shown. Since we have the simple expression $\\nabla f(\\mathbf{x}) = Q\\mathbf{x}-\\mathbf{b}$, one step of steepest descent is given by\n",
    "\n",
    "$$ \\mathbf{x}_{k+1} = \\mathbf{x}_k - \\alpha_k\\nabla f_k = \\mathbf{x}_k - \\alpha_k(Q\\mathbf{x}_k-\\mathbf{b}) $$\n",
    "\n",
    "Then the optimal value of $\\alpha_k$ is easily determined by taking a derivative of $\\phi(\\alpha)=f(\\mathbf{x}_k -\\alpha\\nabla f_k)$ and setting it equal to zero, giving\n",
    "\n",
    "$$ \\begin{align*}\n",
    "    0 &= -\\nabla f_k^T\\big(Q(\\mathbf{x}_k-\\alpha\\nabla f_k)-\\mathbf{b}\\big) \\\\\n",
    "    &= -\\nabla f_k^T(Q\\mathbf{x}_k-\\mathbf{b}) + \\alpha\\nabla f_k^TQ\\nabla f_k \\\\\n",
    "    &= -\\nabla f_k^T\\nabla f_k + \\alpha\\nabla f_k^TQ\\nabla f_k \\\\\n",
    "    \\implies &\\alpha_k = \\frac{\\|\\nabla f_k\\|^2}{\\|\\nabla f_k\\|_Q^2} = \\frac{(Q\\mathbf{x}_k-\\mathbf{b})^T(Q\\mathbf{x}_k-\\mathbf{b})}{(Q\\mathbf{x}_k-\\mathbf{b})^TQ(Q\\mathbf{x}_k-\\mathbf{b})}\n",
    "\\end{align*}$$\n",
    "\n",
    "where $\\|\\mathbf{u}\\|_Q^2 = \\mathbf{u}^TQ\\mathbf{u}$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, concerning rate of convergence, we need to find the ratio between the error in successive iterations,\n",
    "\n",
    "$$ \\frac{\\|\\mathbf{x}_{k+1}-\\mathbf{x}^*\\|}{\\|\\mathbf{x}_{k}-\\mathbf{x}^*\\|} $$\n",
    "\n",
    "Noting that $Q\\mathbf{x}^*=\\mathbf{b}$ by design (i.e. $\\nabla f(\\mathbf{x}^*)=\\mathbf{0}$), we can rewrite one iteration of steepest descent as\n",
    "\n",
    "$$ \\begin{align*}\n",
    "    \\mathbf{x}_{k+1} &= \\mathbf{x}_k - \\alpha_k(Q\\mathbf{x}_k-Q\\mathbf{x}^*) \\\\\n",
    "    \\implies \\mathbf{x}_{k+1}-\\mathbf{x}^* &= \\mathbf{x}_k -\\mathbf{x}^*- \\alpha_kQ(\\mathbf{x}_k-\\mathbf{x}^*) \\\\\n",
    "            &= (I-\\alpha_kQ)(\\mathbf{x}_k-\\mathbf{x}^*) \\\\\n",
    "    \\implies \\|\\mathbf{x}_{k+1}-\\mathbf{x}^*\\|^2 &= (\\mathbf{x}_{k+1}-\\mathbf{x}^*)^T(I-\\alpha_kQ)(\\mathbf{x}_k-\\mathbf{x}^*) \\\\\n",
    "            &=(\\mathbf{x}_k-\\mathbf{x}^*)^T(I-\\alpha_kQ)^2(\\mathbf{x}_k-\\mathbf{x}^*) \\\\\n",
    "    \\implies \\frac{\\|\\mathbf{x}_{k+1}-\\mathbf{x}^*\\|^2}{\\|\\mathbf{x}_{k}-\\mathbf{x}^*\\|^2} &\\le \\lambda_{max}\\big((I-\\alpha_kQ)^2\\big)\n",
    "\\end{align*}$$\n",
    "\n",
    "where the last inequality uses the fact that $\\|\\mathbf{u}\\|_Q\\le\\lambda_{max}(Q)\\|\\mathbf{u}\\|$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It was shown in lecture that this bound is at a minimum when\n",
    "$$ \\alpha_k = \\frac{2}{\\lambda_{max}(Q) + \\lambda_{min}(Q)} $$\n",
    "\n",
    "which is independent of $\\mathbf{x}_k$ and thus is a good choice for fixed step size steepest descent for quadratic functions. Indeed if we substitute this value of $\\alpha$ in, we get the inequality\n",
    "$$ \\frac{\\|\\mathbf{x}_{k+1}-\\mathbf{x}^*\\|^2}{\\|\\mathbf{x}_{k}-\\mathbf{x}^*\\|^2} \\le \\left(\\frac{\\lambda_{max}-\\lambda_{min}}{\\lambda_{max}+\\lambda_{min}}\\right)^2 \\equiv \\left(\\frac{\\kappa-1}{\\kappa+1}\\right)^2 $$\n",
    "\n",
    "where $\\kappa\\equiv \\lambda_{max}/\\lambda_{min}$ is called the **condition number** of $Q$. Note that (the square root of) this inequality tells us steepest descent with this choice of $\\alpha$ convergens **linearly at worst** ($p=1$, $L<1$), and in certain special cases, e.g. when $Q=\\mu I$ so that $\\lambda_{max}=\\lambda_{min}\\implies \\kappa=1$ or when $\\mathbf{x}_0-\\mathbf{x}^*$ is a multiple of an eigenvector of $Q$ (homework exercise), the iteration converges in *a single step*!\n",
    "\n",
    "Indeed though this seems like a very special type of function, the above result can be extended to any function $f$, not necessarily quadratic, so long as $H\\equiv \\nabla^2 f(\\mathbf{x}^*)$ is symmetric positive definite at the minimizer, with the substitution $\\kappa\\to\\kappa_H$, where $\\kappa_H$ is the condition number of $H$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Fixed step steepest descent\n",
    "\n",
    "Perform fixed step steepest descent on the function $f(x,y)=x^2 + 2y^2 + 4x + y + 6$ starting from an initial guess of $\\mathbf{x}_0=(3,-2)$ and choosing $\\alpha_k$ to be the constant defined above.\n",
    "\n",
    "**Solution:** First, note that this is a quadratic function with\n",
    "$$ Q=\\begin{pmatrix}\n",
    "    2 & 0 \\\\\n",
    "    0 & 4\n",
    "\\end{pmatrix},\\quad \\mathbf{b}=\\begin{pmatrix} -4 \\\\ -1\\end{pmatrix} $$\n",
    "\n",
    "Then $\\lambda_{max}(Q)=4$, $\\lambda_{min}(Q)=2$, and thus $\\alpha_k=2/(4+2)=1/3$ should provide adequately fast convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function and gradient\n",
    "f = lambda x: x[0]**2 + 2*x[1]**2 + 4*x[0] + x[1] + 6\n",
    "Df = lambda x: np.array([2*x[0]+4, 4*x[1]+1])\n",
    "\n",
    "x = np.array([3,-2])  # initial point\n",
    "path = [x]\n",
    "print(f'Initial x={x}')\n",
    "alpha = 1/3           # step size\n",
    "tol = 1e-8            # stop when gradient is smaller than this amount\n",
    "max_steps = 1000      # Maximum number of steps to run the iteration\n",
    "i=0                   # iteration count\n",
    "dx = Df(x)            # current gradient\n",
    "while np.linalg.norm(dx)>tol and i<max_steps:\n",
    "    xnew = x - alpha*dx\n",
    "    path.append(xnew)\n",
    "    x = xnew\n",
    "    dx = Df(x)\n",
    "    i += 1\n",
    "\n",
    "path=np.array(path)\n",
    "print(f'After {i} iterations, approximate minimum is {f(x)} at {x}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Investigate what happens for values of $\\alpha$ slightly larger or slightly smaller than the \"optimal\" fixed value of $1/3$. You should see that convergence takes more iterations in each case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "err = np.linalg.norm(path-np.array([-2,-1/4]),axis=1) # ||x_k - x*||\n",
    "print(err[-1]/err[-2])   # limiting convergence bound, should be â‰¤ 1/3 = (K-1)/(K+1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Newton's Method\n",
    "\n",
    "While linear convergence is decent, super-linear convergence ($p>1)$ would be nice, and the holy grail would be convergence in a single step for *all* functions, not just special edge cases. Here, we introduce another line search method, **Newton's method**, which we show shortly will have super-linear (*quadratic*, $p=2$ in this case) convergence, better than steepest descent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You have likely heard of [Newton's method](https://en.wikipedia.org/wiki/Newton's_method) for finding the zeros of a single-variable, nonlinear function, i.e. an $x^*$ such that $f(x^*)=0$. The iteration is given by\n",
    "$$ x_{k+1} = x_k - \\frac{f(x_k)}{f'(x_k)} $$\n",
    "\n",
    "and can be interpreted geometrically as constructing a tangent line to $f$ at $x_k$, finding where the tangent line crosses the $x$-axis, and repeating, as shown in the image below:\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/2/22/Newton_method_scheme.svg/992px-Newton_method_scheme.svg.png\" width=30% />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In optimization problems, we're usually searching for a point where $f'(x)=0$, not $f(x)=0$, so simply replacing $f\\to f'$ yields an algorithm for finding a minimum, albeit requiring information about the second derivative $f''(x)$. We have for [Newton's method in optimization](https://en.wikipedia.org/wiki/Newton%27s_method_in_optimization),\n",
    "\n",
    "$$ x_{k+1} = x_k - \\frac{f'(x_k)}{f''(x_k)} $$\n",
    "or the straightforward generalization to multiple dimensions\n",
    "$$ \\mathbf{x}_{k+1} = \\mathbf{x}_k - \\left[\\nabla^2 f_k\\right]^{-1}\\nabla f_k $$\n",
    "\n",
    "It should be easy to see that this does indeed correspond to a line search algorith with $\\mathbf{p}_k = -\\left[\\nabla^2 f_k\\right]^{-1}$, called the **Newton direction**, and $\\alpha_k=1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now apply Newton's method to the example function $f(x,y)=x^2+2y^2+4x+y+6$ defined above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function, gradient, and now HESSIAN\n",
    "f = lambda x: x[0]**2 + 2*x[1]**2 + 4*x[0] + x[1] + 6\n",
    "Df = lambda x: np.array([2*x[0]+4, 4*x[1]+1])\n",
    "D2f = lambda x: np.array([[2,0],[0,4]])  # technically doesn't have to be a function here, but for uniformity's sake\n",
    "\n",
    "x = np.array([3,-2])  # initial point\n",
    "path = [x]\n",
    "print(f'Initial x={x}')\n",
    "alpha = 1             # step size is 1 in Newton's method\n",
    "tol = 1e-8            # stop when gradient is smaller than this amount\n",
    "max_steps = 1000      # Maximum number of steps to run the iteration\n",
    "i=0                   # iteration count\n",
    "dx = Df(x)            # current gradient\n",
    "while np.linalg.norm(dx)>tol and i<max_steps:\n",
    "    pk = np.linalg.solve(D2f(x),dx)  # faster to solve a system than manually invert\n",
    "    xnew = x - pk\n",
    "    path.append(xnew)\n",
    "    x = xnew\n",
    "    dx = Df(x)\n",
    "    i += 1\n",
    "\n",
    "path=np.array(path)\n",
    "print(f'After {i} iterations, approximate minimum is {f(x)} at {x}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow! Newton's method converged in a single iteration! Indeed, this is the power of Newton's method, at least for quadratic functions. Since for any quadratic function $\\nabla^2 f = Q$, a single iteration sets\n",
    "\n",
    "$$ \\mathbf{x}_1 = \\mathbf{x}_0 - Q^{-1}\\nabla f_0 = \\mathbf{x}_0 - Q^{-1}(Q\\mathbf{x}_0-\\mathbf{b}) = \\mathbf{x}_0 - \\mathbf{x}_0 + Q^{-1}\\mathbf{b} = \\mathbf{x}^* $$\n",
    "\n",
    "since $\\nabla f = Q\\mathbf{x}-\\mathbf{b}$ and thus $Q\\mathbf{x}^*=\\mathbf{b}$. The result is that Newton's method **converges in one iteration** for quadratic functions!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indeed, by a similar argument to the above for steepest descent, it can be shown that the ratio of errors goes to zero, showing that Newton's method **converges quadratically** at worst for any function with $\\nabla^2 f(\\mathbf{x}^*)$ SPD."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drawbacks of Newton's method\n",
    "\n",
    "Although Newton's method does converge more quickly than steepest descent, there are a few drawbacks:\n",
    "\n",
    "1. First and foremost, it requires calculating not just the gradient but also the Hessian (at least ostensibly) by hand.\n",
    "2. Even if we can find a way to numerically calculate the Hessian, Newton's method also requires inverting the Hessian (or, more efficiently, solving a linear system) during each iteration, which can be costly, especially for high-dimensional functions.\n",
    "3. The Hessian may even be singular far away from the minimizer (though it can be shown the Hessian is nonsingular at least in a neighborhood of $\\mathbf{x}^*$), so Newton's method cannot even be defined globally; it is only **locally** convergent.\n",
    "4. Even if the Hessian is nonsingular at each point on the iteration, the Newton direction may not be a descent direction in general.\n",
    "\n",
    "We will talk next time about modifications to Newton's method which attempt to remedy all of the above, called **Quasi-Newton methods**."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
