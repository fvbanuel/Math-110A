{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Math 110A Homework 4\n",
    "### Francisco Banuelos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "a,b = 1,100    # parameters for Rosenbrock function\n",
    "f = lambda x,y: (a-x)**2+b*(y-x**2)**2\n",
    "Df = lambda x,y: np.array([2*(x-a)-4*b*x*(y-x**2),\n",
    "                           2*b*(y-x**2)])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def approx_convergence_rate(path, minimizer, numToAvg=100, showPlot=False):\n",
    "    '''Given a path defined by an iteration and a known minimizer, approximates convergence rate'''\n",
    "    err = np.linalg.norm(path-np.array(minimizer),axis=1) # ||x_k-x*||=e_k\n",
    "    \n",
    "    # if converged in very few steps, return infinite order\n",
    "    if len(err)<=3:\n",
    "        return np.inf\n",
    "    \n",
    "    pp = np.zeros(len(err)-3)\n",
    "    for i in range(len(err)-3):\n",
    "        pp[i] = np.log(err[i+2]/err[i+1])/np.log(err[i+1]/err[i])\n",
    "    \n",
    "    if numToAvg>len(pp):\n",
    "        # if not enough iterations to average, just average all\n",
    "        p=np.mean(pp)\n",
    "    else:\n",
    "        # return mean of last few iterations\n",
    "        p=np.mean(pp[-numToAvg:])\n",
    "        \n",
    "    # plot\n",
    "    if showPlot:\n",
    "        plt.plot(pp)\n",
    "        plt.plot(pp*0+p)\n",
    "        plt.xlabel('k')\n",
    "        plt.ylabel('p')\n",
    "        plt.title(f'p={p}')\n",
    "        plt.show()\n",
    "        \n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PR_beta(Dfk,Dfk1):\n",
    "    return ((Dfk1-Dfk)@Dfk1) / (Dfk@Dfk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FR_beta(Dfk,Dfk1):\n",
    "    return (Dfk1@Dfk1) / (Dfk@Dfk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exact line search means finding the value of $\\alpha$. As for the direction of descent, we choose the negative of the gradient at the inital point. The whole procedure is summarized with the finding the values of $\\alpha$ such that \n",
    "\n",
    "$$\\min_{\\alpha_{k}>0}\\phi(\\alpha)= \\min_{\\alpha>0}f(\\vec{x_{k}}+\\alpha \\vec{p_{k}})$$\n",
    "\n",
    "where $\\vec{p_{k}}=-\\nabla f_{k}$. The initial point is considered at $\\vec{x_{0}}=(1.2,1.2)$ and $\\vec{x_{0}}=(-1.2,1)$\n",
    "\n",
    "To find the optimal value, solve for the critical points. \n",
    "\n",
    "It is found that for the initial point at (1.2,1.2) $\\alpha=0.0122$,$\\alpha=0.0236$, and $\\alpha=0.00076$. These are the roots of:\n",
    "$$\\cssId{diff-var-order-mathjax}{\\tfrac{\\mathrm{d}}{\\mathrm{d}{\\alpha}}}\\left[{100\\left(1.2-200{\\alpha}\\left(1.2-1.2^2\\right)-\\left(1.2+400{\\cdot}1.2{\\alpha}\\left(1.2-1.2^2\\right)+2{\\alpha}\\left(1-1.2\\right)\\right)^2\\right)^2+\\left(1-1.2-400{\\alpha}{\\cdot}1.2\\left(1.2-1.2^2\\right)-2{\\alpha}\\left(1-1.2\\right)\\right)^2}\\right]=0$$\n",
    "\n",
    "\n",
    "The point corresponding the minimum is the last value of $\\alpha$. Hence, this value is used. This is verified using the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0007626486503493179"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.optimize import minimize_scalar\n",
    "x,y=1.2,1.2 # initial point\n",
    "dx=Df(x,y)\n",
    "pk=-dx\n",
    "\n",
    "phi=lambda c:f(x + c*pk[0], y + c*pk[1]) # phi function as given above\n",
    "res = minimize_scalar(phi) # find optimal value\n",
    "res.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After 10000 iterations, approximate minimum is 4.1011671962380484e-05 at (1.0063989536600613, 1.0128643615776578)\n",
      " The approximate convergence rate is 1.0000023316842594\n",
      " The approximate limit L is 0.9997011881242253\n"
     ]
    }
   ],
   "source": [
    "x,y = 1.2,1.2\n",
    "path_PR = [[x,y]]\n",
    "tol = 1e-8            # stop when gradient is smaller than this amount\n",
    "max_steps = 10000     # Maximum number of steps to run the iteration\n",
    "i=0                   # iteration count\n",
    "dx = Df(x,y)          # current gradient\n",
    "pk = -dx\n",
    "while np.linalg.norm(dx)>tol and i<max_steps:    \n",
    "    \n",
    "    alpha = 0.00076 # alpha from exact line search\n",
    "    \n",
    "\n",
    "    xnew,ynew = x + alpha*pk[0], y + alpha*pk[1]\n",
    "    dx1 = Df(xnew,ynew)      # Df_{k+1}\n",
    "    bk = PR_beta(dx,dx1)     # beta_k\n",
    "    \n",
    "    pk = -dx1 + bk*pk\n",
    "    \n",
    "    path_PR.append([xnew,ynew])\n",
    "    x,y = xnew,ynew\n",
    "    dx = dx1\n",
    "    i += 1\n",
    "\n",
    "path_PR=np.array(path_PR)\n",
    "print(f'After {i} iterations, approximate minimum is {f(x,y)} at {x,y}')\n",
    "R4=approx_convergence_rate(path_PR, [1,1], numToAvg=100, showPlot=False)\n",
    "err=np.linalg.norm(np.diff(path_PR,axis=0),axis=1)\n",
    "L=err[-1]/err[-2] # Approximate limit L of the ratio of the erros\n",
    "print(f' The approximate convergence rate is {R4}')\n",
    "print(f' The approximate limit L is {L}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After 964 iterations, approximate minimum is 1.2462183404613065e-16 at (1.0000000111542098, 1.000000022353745)\n",
      " The approximate convergence rate is 1.0000043372524952\n",
      " Approximate limit is 0.9788417693199295\n"
     ]
    }
   ],
   "source": [
    "x,y = 1.2,1.2\n",
    "path_FR = [[x,y]]\n",
    "tol = 1e-8            # stop when gradient is smaller than this amount\n",
    "max_steps = 10000     # Maximum number of steps to run the iteration\n",
    "i=0                   # iteration count\n",
    "dx = Df(x,y)          # current gradient\n",
    "pk = -dx\n",
    "while np.linalg.norm(dx)>tol and i<max_steps:    \n",
    "    \n",
    "    alpha = 0.00076 # alpha from exact line search\n",
    "\n",
    "    xnew,ynew = x + alpha*pk[0], y + alpha*pk[1]\n",
    "    dx1 = Df(xnew,ynew)      # Df_{k+1}\n",
    "    bk = FR_beta(dx,dx1)  # beta_k\n",
    "    \n",
    "    pk = -dx1 + bk*pk\n",
    "    \n",
    "    path_FR.append([xnew,ynew])\n",
    "    x,y = xnew,ynew\n",
    "    dx = dx1\n",
    "    i += 1\n",
    "\n",
    "path_FR=np.array(path_FR)\n",
    "print(f'After {i} iterations, approximate minimum is {f(x,y)} at {x,y}')\n",
    "R5=approx_convergence_rate(path_FR, [1,1], numToAvg=100, showPlot=False)\n",
    "err=np.linalg.norm(np.diff(path_FR,axis=0),axis=1)\n",
    "L=err[-1]/err[-2] # Approximate limit L of the ratio of errors\n",
    "print(f' The approximate convergence rate is {R5}')\n",
    "print(f' Approximate limit is {L}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is seen that both converge successfully to the minimizer and their approximate convergence rate is $1$. Furthermore, since $L<1$, this confirms that both converge linearly but Fletcher-Reeves is faster since it only took $964$ iterations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we restart every 3 iterations. That means setting $\\beta_{k}=0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After 10000 iterations, approximate minimum is 4.09856790792875e-05 at (1.0063969255224006, 1.0128602712970445)\n",
      " The approximate convergence rate is 0.999999372600341\n",
      " The approximate limit L is 0.9997013653917546 \n"
     ]
    }
   ],
   "source": [
    "x,y = 1.2,1.2\n",
    "path_PR = [[x,y]]\n",
    "tol = 1e-8            # stop when gradient is smaller than this amount\n",
    "max_steps = 10000     # Maximum number of steps to run the iteration\n",
    "i=0                   # iteration count\n",
    "dx = Df(x,y)          # current gradient\n",
    "pk = -dx\n",
    "while np.linalg.norm(dx)>tol and i<max_steps:    \n",
    "    \n",
    "    alpha = 0.00076 # alpha from exact line search\n",
    "    \n",
    "\n",
    "    xnew,ynew = x + alpha*pk[0], y + alpha*pk[1]\n",
    "    dx1 = Df(xnew,ynew)      # Df_{k+1}\n",
    "    bk = PR_beta(dx,dx1)     # beta_k\n",
    "    if i%3 == 0:     # restart every 3 iterations\n",
    "        bk=0\n",
    "    pk = -dx1 + bk*pk\n",
    "    \n",
    "    path_PR.append([xnew,ynew])\n",
    "    x,y = xnew,ynew\n",
    "    dx = dx1\n",
    "    i += 1\n",
    "\n",
    "path_PR=np.array(path_PR)\n",
    "print(f'After {i} iterations, approximate minimum is {f(x,y)} at {x,y}')\n",
    "R=approx_convergence_rate(path_PR, [1,1], numToAvg=100, showPlot=False)\n",
    "err=np.linalg.norm(np.diff(path_PR,axis=0),axis=1)\n",
    "L=err[-1]/err[-2] # Limit L of the erros\n",
    "print(f' The approximate convergence rate is {R}')\n",
    "print(f' The approximate limit L is {L} ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After 10000 iterations, approximate minimum is 1.5135521143124254e-07 at (1.000388570769531, 1.000779210776803)\n",
      " The approximate convergence rate is 1.2644368369249688\n",
      " The approximate limit L is 1.553674087780001 \n"
     ]
    }
   ],
   "source": [
    "x,y = 1.2,1.2\n",
    "path_FR = [[x,y]]\n",
    "tol = 1e-8            # stop when gradient is smaller than this amount\n",
    "max_steps = 10000     # Maximum number of steps to run the iteration\n",
    "i=0                   # iteration count\n",
    "dx = Df(x,y)          # current gradient\n",
    "pk = -dx\n",
    "while np.linalg.norm(dx)>tol and i<max_steps:    \n",
    "    \n",
    "    alpha = 0.00076\n",
    "\n",
    "    xnew,ynew = x + alpha*pk[0], y + alpha*pk[1]\n",
    "    dx1 = Df(xnew,ynew)      # Df_{k+1}\n",
    "    bk = FR_beta(dx,dx1)  # beta_k\n",
    "    if i%3 == 0:     # restarting every 3 iterations\n",
    "        bk=0\n",
    "    pk = -dx1 + bk*pk\n",
    "    \n",
    "    path_FR.append([xnew,ynew])\n",
    "    x,y = xnew,ynew\n",
    "    dx = dx1\n",
    "    i += 1\n",
    "\n",
    "path_FR=np.array(path_FR)\n",
    "print(f'After {i} iterations, approximate minimum is {f(x,y)} at {x,y}')\n",
    "R1=approx_convergence_rate(path_FR, [1,1], numToAvg=100, showPlot=False)\n",
    "err=np.linalg.norm(np.diff(path_FR,axis=0),axis=1)\n",
    "L=err[-1]/err[-2]\n",
    "print(f' The approximate convergence rate is {R1}')\n",
    "print(f' The approximate limit L is {L} ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The convergence is nearly linear for the PR method. For Fletcher-Reeves, the convergence is super linear which is an improvement from Fletcher-Reeves without restarting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second point,at $(-1.2,1)$, has 3 critical points:$\\alpha=0.0122$, $\\alpha=0.0650$, and $\\alpha=0.00079$. These are the roots of:\n",
    "\n",
    "$$\\cssId{diff-var-order-mathjax}{\\tfrac{\\mathrm{d}}{\\mathrm{d}{\\alpha}}}\\left[{100\\left(1-200{\\alpha}\\left(1-1.2^2\\right)-\\left(-1.2-400{\\cdot}1.2{\\alpha}\\left(1-1.2^2\\right)+2{\\alpha}\\left(1+1.2\\right)\\right)^2\\right)^2+\\left(1+1.2+400{\\alpha}{\\cdot}1.2\\left(1-1.2^2\\right)-2{\\alpha}\\left(1+1.2\\right)\\right)^2}\\right]=0$$\n",
    "\n",
    "The value of $\\alpha$ satisfying the minimization is $\\alpha=0.00079$. This is verified in the code below.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0007880024518644726"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.optimize import minimize_scalar\n",
    "x,y=-1.2,1 # initial value\n",
    "dx=Df(x,y)\n",
    "pk=-dx\n",
    "\n",
    "phi=lambda c:f(x + c*pk[0], y + c*pk[1]) # phi function to be minimized\n",
    "res = minimize_scalar(phi) # minimize\n",
    "res.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After 10000 iterations, approximate minimum is 0.0003254246072593479 at (0.9819752124736729, 0.9642024041355456)\n",
      " The approximate convergence rate is 0.9999928043011022\n",
      " The approximate limit L is 0.999670612325752\n"
     ]
    }
   ],
   "source": [
    "x,y = -1.2,1 # initial point\n",
    "path_PR = [[x,y]]\n",
    "tol = 1e-8            # stop when gradient is smaller than this amount\n",
    "max_steps = 10000     # Maximum number of steps to run the iteration\n",
    "i=0                   # iteration count\n",
    "dx = Df(x,y)          # current gradient\n",
    "pk = -dx\n",
    "while np.linalg.norm(dx)>tol and i<max_steps:    \n",
    "    \n",
    "    alpha = 0.00079 # alpha from exact line search\n",
    "    \n",
    "\n",
    "    xnew,ynew = x + alpha*pk[0], y + alpha*pk[1]\n",
    "    dx1 = Df(xnew,ynew)      # Df_{k+1}\n",
    "    bk = PR_beta(dx,dx1)     # beta_k\n",
    "    \n",
    "    pk = -dx1 + bk*pk\n",
    "    \n",
    "    path_PR.append([xnew,ynew])\n",
    "    x,y = xnew,ynew\n",
    "    dx = dx1\n",
    "    i += 1\n",
    "\n",
    "path_PR=np.array(path_PR)\n",
    "print(f'After {i} iterations, approximate minimum is {f(x,y)} at {x,y}')\n",
    "R2=approx_convergence_rate(path_PR, [1,1], numToAvg=100, showPlot=False)\n",
    "err=np.linalg.norm(np.diff(path_PR,axis=0),axis=1)\n",
    "L=err[-1]/err[-2]\n",
    "print(f' The approximate convergence rate is {R2}')\n",
    "print(f' The approximate limit L is {L}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After 37 iterations, approximate minimum is nan at (nan, nan)\n",
      " The approximate convergence rate is inf\n",
      " The approximate limit L is nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:3: RuntimeWarning: overflow encountered in double_scalars\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:4: RuntimeWarning: overflow encountered in double_scalars\n",
      "  after removing the cwd from sys.path.\n",
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:16: RuntimeWarning: invalid value encountered in add\n",
      "  app.launch_new_instance()\n",
      "/opt/anaconda3/lib/python3.7/site-packages/numpy/linalg/linalg.py:2512: RuntimeWarning: overflow encountered in multiply\n",
      "  s = (x.conj() * x).real\n"
     ]
    }
   ],
   "source": [
    "x,y = -1.2,1 # initial point\n",
    "path_FR = [[x,y]]\n",
    "tol = 1e-8            # stop when gradient is smaller than this amount\n",
    "max_steps = 10000     # Maximum number of steps to run the iteration\n",
    "i=0                   # iteration count\n",
    "dx = Df(x,y)          # current gradient\n",
    "pk = -dx\n",
    "while np.linalg.norm(dx)>tol and i<max_steps:    \n",
    "    \n",
    "    alpha = 0.00079\n",
    "\n",
    "    xnew,ynew = x + alpha*pk[0], y + alpha*pk[1]\n",
    "    dx1 = Df(xnew,ynew)      # Df_{k+1}\n",
    "    bk = FR_beta(dx,dx1)  # beta_k\n",
    "   \n",
    "    pk = -dx1 + bk*pk\n",
    "    \n",
    "    path_FR.append([xnew,ynew])\n",
    "    x,y = xnew,ynew\n",
    "    dx = dx1\n",
    "    i += 1\n",
    "\n",
    "path_FR=np.array(path_FR)\n",
    "print(f'After {i} iterations, approximate minimum is {f(x,y)} at {x,y}')\n",
    "R3=approx_convergence_rate(path_FR, [1,1], numToAvg=100, showPlot=False)\n",
    "err=np.linalg.norm(np.diff(path_FR,axis=0),axis=1)\n",
    "L=err[-1]/err[-2]\n",
    "print(f' The approximate convergence rate is {R3}')\n",
    "print(f' The approximate limit L is {L}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is seen that the PR method is very close to being linear or even sub-linear. The Fletcher-Reeves method diverges from this point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After 10000 iterations, approximate minimum is 0.0003250974444066023 at (0.9819842751250115, 0.9642202398725522)\n",
      " The approximate convergence rate is 0.9999895428800052\n",
      " The approximate limit L is 0.9996708367290676\n"
     ]
    }
   ],
   "source": [
    "x,y = -1.2,1 # initial point\n",
    "path_PR = [[x,y]]\n",
    "tol = 1e-8            # stop when gradient is smaller than this amount\n",
    "max_steps = 10000     # Maximum number of steps to run the iteration\n",
    "i=0                   # iteration count\n",
    "dx = Df(x,y)          # current gradient\n",
    "pk = -dx\n",
    "while np.linalg.norm(dx)>tol and i<max_steps:    \n",
    "    \n",
    "    alpha = 0.00079 # alpha from exact line search\n",
    "    \n",
    "\n",
    "    xnew,ynew = x + alpha*pk[0], y + alpha*pk[1]\n",
    "    dx1 = Df(xnew,ynew)      # Df_{k+1}\n",
    "    bk = PR_beta(dx,dx1)     # beta_k\n",
    "    if i%3 == 0:     # restarting every 3 iterations\n",
    "        bk=0\n",
    "    pk = -dx1 + bk*pk\n",
    "    \n",
    "    path_PR.append([xnew,ynew])\n",
    "    x,y = xnew,ynew\n",
    "    dx = dx1\n",
    "    i += 1\n",
    "\n",
    "path_PR=np.array(path_PR)\n",
    "print(f'After {i} iterations, approximate minimum is {f(x,y)} at {x,y}')\n",
    "R2=approx_convergence_rate(path_PR, [1,1], numToAvg=100, showPlot=False)\n",
    "err=np.linalg.norm(np.diff(path_PR,axis=0),axis=1)\n",
    "L=err[-1]/err[-2]\n",
    "print(f' The approximate convergence rate is {R2}')\n",
    "print(f' The approximate limit L is {L}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After 10000 iterations, approximate minimum is 7.848259836416086e-07 at (0.9991145040972794, 0.9982271034503895)\n",
      " The approximate convergence rate is 1.2659157565027892\n",
      " The approximate limit L is 1.5700289936439011\n"
     ]
    }
   ],
   "source": [
    "x,y = -1.2,1 # initial point\n",
    "path_FR = [[x,y]]\n",
    "tol = 1e-8            # stop when gradient is smaller than this amount\n",
    "max_steps = 10000     # Maximum number of steps to run the iteration\n",
    "i=0                   # iteration count\n",
    "dx = Df(x,y)          # current gradient\n",
    "pk = -dx\n",
    "while np.linalg.norm(dx)>tol and i<max_steps:    \n",
    "    \n",
    "    alpha = 0.00079\n",
    "\n",
    "    xnew,ynew = x + alpha*pk[0], y + alpha*pk[1]\n",
    "    dx1 = Df(xnew,ynew)      # Df_{k+1}\n",
    "    bk = FR_beta(dx,dx1)  # beta_k\n",
    "    if i%3 == 0:     # restarting every 3 iterations\n",
    "        bk=0\n",
    "    pk = -dx1 + bk*pk\n",
    "    \n",
    "    path_FR.append([xnew,ynew])\n",
    "    x,y = xnew,ynew\n",
    "    dx = dx1\n",
    "    i += 1\n",
    "\n",
    "path_FR=np.array(path_FR)\n",
    "print(f'After {i} iterations, approximate minimum is {f(x,y)} at {x,y}')\n",
    "R3=approx_convergence_rate(path_FR, [1,1], numToAvg=100, showPlot=False)\n",
    "err=np.linalg.norm(np.diff(path_FR,axis=0),axis=1)\n",
    "L=err[-1]/err[-2]\n",
    "print(f' The approximate convergence rate is {R3}')\n",
    "print(f' The approximate limit L is {L}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Restarting every  3 iterations, the convergence rate for the PR method remains very much unchanged. However, the situation improves for the Fletcher-Reeves as it no longer diverges and it converges in a superlinear fashion. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
